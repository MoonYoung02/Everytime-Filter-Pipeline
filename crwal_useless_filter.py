import re
import json
from datetime import datetime
from pathlib import Path
from playwright.sync_api import sync_playwright
from dicts import USELESS_KEYWORDS

STATE_PATH = "everytime_state.json"
START_URL = "https://everytime.kr/384921"   # ììœ ê²Œì‹œíŒ - current: 30
PAGES_TO_SCRAPE = 30                         # ëª‡ í˜ì´ì§€ê¹Œì§€ ì €ì¥í• ì§€
OUT_PATH = Path("everytime_posts_filtered.jsonl")    # ì¤„ë‹¨ìœ„ JSON ì €ì¥

def extract_posts_on_page(page):
    """
    í˜„ì¬ í˜ì´ì§€ì˜ ê¸€ ëª©ë¡ì—ì„œ ì œëª©/ìš”ì•½/ì‹œê°„/ë§í¬ë¥¼ ì¶”ì¶œí•´ì„œ list[dict]ë¡œ ë°˜í™˜
    HTML êµ¬ì¡°(ë„¤ê°€ ì¤€ ê²ƒ) ê¸°ì¤€:
      div.wrap.articles > article.list > a.article > div.desc > h2 + p + div.info(time)
    """
    posts = []
    items = page.locator("div.wrap.articles article.list a.article")
    count = items.count()

    for i in range(count):
        a = items.nth(i)

        title = a.locator("h2.medium.bold").first.inner_text().strip()
        # ğŸš« ì œëª©ì— ì“¸ëª¨ì—†ëŠ” í‚¤ì›Œë“œê°€ í¬í•¨ë˜ë©´ ìŠ¤í‚µ
        if is_useless_title(title):
            print(f"[SKIP] {title}")
            continue
        snippet = a.locator("p.medium").first.inner_text().strip()

        # ì‹œê°„ì€ ì—†ëŠ” ê¸€ë„ ìˆì„ ìˆ˜ ìˆì–´ì„œ ì•ˆì „í•˜ê²Œ
        time_loc = a.locator("time.small")
        post_time = time_loc.first.inner_text().strip() if time_loc.count() else ""

        href = a.get_attribute("href") or ""
        # hrefê°€ /384921/v/... í˜•íƒœì´ë¯€ë¡œ ì ˆëŒ€ URLë¡œ ë°”ê¿” ì €ì¥
        abs_url = page.url.split("/p/")[0].rstrip("/")  # https://everytime.kr/384921 ë˜ëŠ” .../p/n ì œê±°
        if href.startswith("/"):
            full_url = "https://everytime.kr" + href
        else:
            full_url = href

        posts.append({
            "board_url": page.url,
            "post_url": full_url,
            "title": title,
            "snippet": snippet,
            "time": post_time,
            "fetched_at": datetime.now().isoformat(timespec="seconds"),
        })
    return posts

def is_useless_title(title: str) -> bool:
    title_lower = title.lower()
    return any(k.lower() in title_lower for k in USELESS_KEYWORDS)


def append_jsonl(path: Path, rows):
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("a", encoding="utf-8") as f:
        for r in rows:
            f.write(json.dumps(r, ensure_ascii=False) + "\n")

with sync_playwright() as p:
    browser = p.chromium.launch(headless=False, slow_mo=1000)  # ì²œì²œíˆ ë³´ì´ê²Œ
    context = browser.new_context(storage_state=STATE_PATH)

    # íŒì—…/alert ê°™ì€ ê²Œ ëœ¨ë©´ ë‹«ì•„ë²„ë¦¬ê¸°(ì•ˆì „ì¥ì¹˜)
    context.on("dialog", lambda d: d.dismiss())

    page = context.new_page()
    page.goto(START_URL, wait_until="domcontentloaded")

    for page_idx in range(1, PAGES_TO_SCRAPE + 1):
        # ëª©ë¡ ë¡œë”© ë³´ì¥: ê¸€ ë¦¬ìŠ¤íŠ¸ê°€ ìµœì†Œ 1ê°œ ëœ° ë•Œê¹Œì§€
        page.locator("div.wrap.articles article.list").first.wait_for(timeout=15_000)

        posts = extract_posts_on_page(page)
        append_jsonl(OUT_PATH, posts)

        print(f"[{page_idx}] saved {len(posts)} posts from: {page.url}")

        # ê° í˜ì´ì§€ ì ê¹ ë©ˆì¶°ì„œ ëˆˆìœ¼ë¡œ í™•ì¸
        # page.wait_for_timeout(2500)

        # ë§ˆì§€ë§‰ í˜ì´ì§€ë©´ ì¢…ë£Œ
        if page_idx == PAGES_TO_SCRAPE:
            break

        # "ë‹¤ìŒ" í´ë¦­í•´ì„œ ì´ë™ (ë„¤ HTML: <a class="next">ë‹¤ìŒ</a>)
        next_link = page.locator("div.pagination a.next")
        if next_link.count() == 0:
            print("ë‹¤ìŒ ë§í¬ë¥¼ ëª» ì°¾ìŒ. ì¢…ë£Œ.")
            break

        next_link.scroll_into_view_if_needed()
        next_link.click()

        # URLì´ /p/<ìˆ«ì> í˜•íƒœë¡œ ë°”ë€ŒëŠ” ê±¸ ê¸°ë‹¤ë¦¼
        page.wait_for_url(re.compile(r".*/p/\d+"), timeout=15_000)
        page.wait_for_load_state("domcontentloaded")

    print(f"Done. Output: {OUT_PATH.resolve()}")

